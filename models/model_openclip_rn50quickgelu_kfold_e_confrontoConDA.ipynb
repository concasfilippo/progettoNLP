{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Baseline per il confronto con la varianete con data augmentation con t-test",
   "id": "710703d4d4e2b8a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:37:37.942053Z",
     "start_time": "2025-05-07T15:05:21.148841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import open_clip\n",
    "\n",
    "\n",
    "DATA_DIR  = \"../pre_processing/dataset_train_e_val_assieme/train/\"\n",
    "TEST_DIR  = \"../pre_processing/dataset_train_e_val_assieme/test/\"\n",
    "TRAIN_JSON = \"../pre_processing/dataset_train_e_val_assieme/train.json\"\n",
    "TEST_JSON  = \"../pre_processing/dataset_train_e_val_assieme/test.json\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 10\n",
    "CV_K       = 5\n",
    "SEED       = 42\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, annotations, img_folder, label_encoder, transform=None):\n",
    "        self.annotations   = annotations\n",
    "        self.img_folder    = img_folder\n",
    "        self.label_encoder = label_encoder\n",
    "        self.transform     = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)), transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item       = self.annotations[idx]\n",
    "        text       = item[\"text\"]\n",
    "        label_name = item[\"label\"]\n",
    "        img_file   = item[\"image\"]\n",
    "        img_path   = os.path.join(self.img_folder, label_name, img_file)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.label_encoder.transform([label_name])[0]\n",
    "        return text, image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "all_labels    = [a[\"label\"] for a in annotations]\n",
    "label_encoder = LabelEncoder().fit(all_labels)\n",
    "\n",
    "\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"RN50-quickgelu\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(\"RN50\")\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze ultimi layer per fine-tuning \n",
    "for name, param in list(clip_model.named_parameters())[-10:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "EMBED_DIM = 1024 + 1024 # concat(text | image)\n",
    "\n",
    "\n",
    "def run_epoch(classifier, loader, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    classifier.train() if is_train else classifier.eval()\n",
    "\n",
    "    epoch_loss  = 0.0\n",
    "    all_preds   = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for texts, images, labels in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_tokens = tokenizer(texts).to(DEVICE)\n",
    "                text_embeds = clip_model.encode_text(text_tokens)\n",
    "                image_embeds = clip_model.encode_image(images)\n",
    "            feats = torch.cat([text_embeds, image_embeds], dim=1)\n",
    "\n",
    "            logits = classifier(feats).squeeze()\n",
    "            loss   = criterion(logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = (logits > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    metrics = {\n",
    "        \"acc\" : accuracy_score(all_targets, all_preds),\n",
    "        \"prec\": precision_score(all_targets, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"rec\" : recall_score(all_targets, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1\"  : f1_score(all_targets, all_preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "    return epoch_loss, metrics\n",
    "\n",
    "def run_test(test_annotations_path, data_dir, classifier):\n",
    "    with open(test_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        test_ann = json.load(f)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MultimodalDataset(test_ann, data_dir, label_encoder, transform=preprocess),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    classifier.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            text_tokens = tokenizer(texts).to(DEVICE)\n",
    "            text_embeds = clip_model.encode_text(text_tokens)\n",
    "            image_embeds = clip_model.encode_image(images.to(DEVICE))\n",
    "            feats = torch.cat([text_embeds, image_embeds], dim=1)\n",
    "            logits = classifier(feats).squeeze()\n",
    "            preds = (logits > 0.5).float().cpu()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(labels.tolist())\n",
    "\n",
    "    acc  = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(all_targets, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(all_targets, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(\"\\n===== TEST RESULTS =====\")\n",
    "    print(f\"Accuracy          : {acc:.4f}\")\n",
    "    print(f\"Precision (macro) : {prec:.4f}\")\n",
    "    print(f\"Recall   (macro)  : {rec:.4f}\")\n",
    "    print(f\"F1-score (macro)  : {f1:.4f}\")\n",
    "\n",
    "# ========= K-fold training =========\n",
    "print(f\"Running {CV_K}-fold CV on {len(annotations)} samples â€¦\")\n",
    "skf = StratifiedKFold(n_splits=CV_K, shuffle=True, random_state=SEED)\n",
    "writer = SummaryWriter(log_dir=f\"runs/cv_{CV_K}_folds\")\n",
    "fold_results = []\n",
    "test_metrics_all = []\n",
    "classifiers = []\n",
    "\n",
    "def tb_tag(fold, metric):\n",
    "    return f\"Fold{fold}/{metric}\"\n",
    "\n",
    "classifier = None\n",
    "\n",
    "for fold_idx, (train_ids, val_ids) in enumerate(skf.split(np.zeros(len(all_labels)), all_labels), 1):\n",
    "    print(f\"\\n=== Fold {fold_idx}/{CV_K} ===\")\n",
    "\n",
    "    train_ann = [annotations[i] for i in train_ids]\n",
    "    val_ann   = [annotations[i] for i in val_ids]\n",
    "\n",
    "    train_loader = DataLoader(MultimodalDataset(train_ann, DATA_DIR, label_encoder, transform=preprocess), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(MultimodalDataset(val_ann,   DATA_DIR, label_encoder, transform=preprocess), batch_size=BATCH_SIZE)\n",
    "\n",
    "    classifier = MultimodalClassifier(EMBED_DIM).to(DEVICE)\n",
    "    criterion  = nn.BCELoss()\n",
    "    optimizer  = torch.optim.Adam(classifier.parameters(), lr=1e-2, weight_decay=0.0001)\n",
    "    scheduler  = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.9, total_iters=EPOCHS)\n",
    "    \n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_m = run_epoch(classifier, train_loader, criterion, optimizer)\n",
    "        val_loss,   val_m   = run_epoch(classifier, val_loader,   criterion)\n",
    "\n",
    "        writer.add_scalar(tb_tag(fold_idx, \"Loss/Train\"), train_loss, epoch)\n",
    "        writer.add_scalar(tb_tag(fold_idx, \"Loss/Val\"),   val_loss,   epoch)\n",
    "        writer.add_scalar(tb_tag(fold_idx, \"F1/Val\"),     val_m[\"f1\"], epoch)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"[Fold {fold_idx} | Ep {epoch:02d}]  Train-loss: {train_loss:.4f} | Val-loss: {val_loss:.4f} | Val F1: {val_m['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        best_f1 = max(best_f1, val_m[\"f1\"])\n",
    "        scheduler.step()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train loss\")\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses,   label=\"Val loss\")\n",
    "    plt.title(f\"Loss curves â€“ Fold {fold_idx}\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Binary cross-entropy\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "    plt.savefig(f\"loss_fold{fold_idx}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    fold_results.append({\"fold\": fold_idx, **val_m, \"best_f1\": best_f1})\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\n======== Cross-validation summary ========\")\n",
    "print(f\"Mean best-F1 across folds: {np.mean([fr['best_f1'] for fr in fold_results]):.4f}\")\n",
    "print(f\"Mean accuracy            : {np.mean([fr['acc'] for fr in fold_results]):.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n======== Test set evaluation (per fold) ========\")\n",
    "for i, clf in enumerate(classifiers, 1):\n",
    "    print(f\"\\n--- Fold {i} ---\")\n",
    "    run_test(TEST_JSON, TEST_DIR, clf)\n",
    "\n",
    "test_metrics_all.append(run_test(TEST_JSON, TEST_DIR, classifier))"
   ],
   "id": "309d9438389c135",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold CV on 1350 samples â€¦\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "[Fold 1 | Ep 01]  Train-loss: 0.5394 | Val-loss: 0.4442 | Val F1: 0.7690\n",
      "[Fold 1 | Ep 02]  Train-loss: 0.3707 | Val-loss: 0.5132 | Val F1: 0.7727\n",
      "[Fold 1 | Ep 03]  Train-loss: 0.3064 | Val-loss: 0.6058 | Val F1: 0.7310\n",
      "[Fold 1 | Ep 04]  Train-loss: 0.2157 | Val-loss: 0.5251 | Val F1: 0.7596\n",
      "[Fold 1 | Ep 05]  Train-loss: 0.1773 | Val-loss: 0.5829 | Val F1: 0.7789\n",
      "[Fold 1 | Ep 06]  Train-loss: 0.1613 | Val-loss: 0.7717 | Val F1: 0.7516\n",
      "[Fold 1 | Ep 07]  Train-loss: 0.1890 | Val-loss: 0.5467 | Val F1: 0.7753\n",
      "[Fold 1 | Ep 08]  Train-loss: 0.1339 | Val-loss: 0.6132 | Val F1: 0.7963\n",
      "[Fold 1 | Ep 09]  Train-loss: 0.1025 | Val-loss: 0.6598 | Val F1: 0.7459\n",
      "[Fold 1 | Ep 10]  Train-loss: 0.1353 | Val-loss: 1.1022 | Val F1: 0.7619\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "[Fold 2 | Ep 01]  Train-loss: 0.5352 | Val-loss: 0.5494 | Val F1: 0.6667\n",
      "[Fold 2 | Ep 02]  Train-loss: 0.3694 | Val-loss: 0.5148 | Val F1: 0.7491\n",
      "[Fold 2 | Ep 03]  Train-loss: 0.2838 | Val-loss: 0.5318 | Val F1: 0.7495\n",
      "[Fold 2 | Ep 04]  Train-loss: 0.1973 | Val-loss: 1.2250 | Val F1: 0.6859\n",
      "[Fold 2 | Ep 05]  Train-loss: 0.2400 | Val-loss: 0.8367 | Val F1: 0.6880\n",
      "[Fold 2 | Ep 06]  Train-loss: 0.2100 | Val-loss: 0.7195 | Val F1: 0.7159\n",
      "[Fold 2 | Ep 07]  Train-loss: 0.1289 | Val-loss: 0.6954 | Val F1: 0.7266\n",
      "[Fold 2 | Ep 08]  Train-loss: 0.1211 | Val-loss: 0.7747 | Val F1: 0.7652\n",
      "[Fold 2 | Ep 09]  Train-loss: 0.1254 | Val-loss: 0.8313 | Val F1: 0.7390\n",
      "[Fold 2 | Ep 10]  Train-loss: 0.1046 | Val-loss: 0.8713 | Val F1: 0.7090\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "[Fold 3 | Ep 01]  Train-loss: 0.5168 | Val-loss: 0.5156 | Val F1: 0.7130\n",
      "[Fold 3 | Ep 02]  Train-loss: 0.3227 | Val-loss: 0.6012 | Val F1: 0.7389\n",
      "[Fold 3 | Ep 03]  Train-loss: 0.2738 | Val-loss: 0.6918 | Val F1: 0.7261\n",
      "[Fold 3 | Ep 04]  Train-loss: 0.2130 | Val-loss: 0.6853 | Val F1: 0.7337\n",
      "[Fold 3 | Ep 05]  Train-loss: 0.1648 | Val-loss: 0.8346 | Val F1: 0.7022\n",
      "[Fold 3 | Ep 06]  Train-loss: 0.1734 | Val-loss: 1.0954 | Val F1: 0.7090\n",
      "[Fold 3 | Ep 07]  Train-loss: 0.1651 | Val-loss: 0.9486 | Val F1: 0.6962\n",
      "[Fold 3 | Ep 08]  Train-loss: 0.1121 | Val-loss: 0.8987 | Val F1: 0.7178\n",
      "[Fold 3 | Ep 09]  Train-loss: 0.1171 | Val-loss: 1.0011 | Val F1: 0.7406\n",
      "[Fold 3 | Ep 10]  Train-loss: 0.0902 | Val-loss: 1.0637 | Val F1: 0.6947\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "[Fold 4 | Ep 01]  Train-loss: 0.5579 | Val-loss: 0.4116 | Val F1: 0.8013\n",
      "[Fold 4 | Ep 02]  Train-loss: 0.4034 | Val-loss: 0.4330 | Val F1: 0.8099\n",
      "[Fold 4 | Ep 03]  Train-loss: 0.3155 | Val-loss: 0.4007 | Val F1: 0.8083\n",
      "[Fold 4 | Ep 04]  Train-loss: 0.2289 | Val-loss: 0.5509 | Val F1: 0.7587\n",
      "[Fold 4 | Ep 05]  Train-loss: 0.1754 | Val-loss: 0.5012 | Val F1: 0.8036\n",
      "[Fold 4 | Ep 06]  Train-loss: 0.1630 | Val-loss: 0.6725 | Val F1: 0.7576\n",
      "[Fold 4 | Ep 07]  Train-loss: 0.1807 | Val-loss: 0.7170 | Val F1: 0.7248\n",
      "[Fold 4 | Ep 08]  Train-loss: 0.1981 | Val-loss: 0.5148 | Val F1: 0.7929\n",
      "[Fold 4 | Ep 09]  Train-loss: 0.1372 | Val-loss: 0.5855 | Val F1: 0.8013\n",
      "[Fold 4 | Ep 10]  Train-loss: 0.1749 | Val-loss: 0.6114 | Val F1: 0.7657\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "[Fold 5 | Ep 01]  Train-loss: 0.5523 | Val-loss: 0.4289 | Val F1: 0.7272\n",
      "[Fold 5 | Ep 02]  Train-loss: 0.3769 | Val-loss: 0.4058 | Val F1: 0.7795\n",
      "[Fold 5 | Ep 03]  Train-loss: 0.2874 | Val-loss: 0.4662 | Val F1: 0.7465\n",
      "[Fold 5 | Ep 04]  Train-loss: 0.2409 | Val-loss: 0.4757 | Val F1: 0.7345\n",
      "[Fold 5 | Ep 05]  Train-loss: 0.1903 | Val-loss: 0.4320 | Val F1: 0.7697\n",
      "[Fold 5 | Ep 06]  Train-loss: 0.1560 | Val-loss: 0.4404 | Val F1: 0.7785\n",
      "[Fold 5 | Ep 07]  Train-loss: 0.1181 | Val-loss: 0.6752 | Val F1: 0.7456\n",
      "[Fold 5 | Ep 08]  Train-loss: 0.1733 | Val-loss: 0.4730 | Val F1: 0.7976\n",
      "[Fold 5 | Ep 09]  Train-loss: 0.1318 | Val-loss: 0.4947 | Val F1: 0.7890\n",
      "[Fold 5 | Ep 10]  Train-loss: 0.1479 | Val-loss: 0.4753 | Val F1: 0.7949\n",
      "\n",
      "======== Cross-validation summary ========\n",
      "Mean best-F1 across folds: 0.7819\n",
      "Mean accuracy            : 0.7800\n",
      "\n",
      "======== Test set evaluation (per fold) ========\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:09<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.8033\n",
      "Precision (macro) : 0.7983\n",
      "Recall   (macro)  : 0.7425\n",
      "F1-score (macro)  : 0.7587\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:09<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.7967\n",
      "Precision (macro) : 0.7790\n",
      "Recall   (macro)  : 0.7475\n",
      "F1-score (macro)  : 0.7587\n",
      "\n",
      "--- Fold 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:09<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.7900\n",
      "Precision (macro) : 0.7639\n",
      "Recall   (macro)  : 0.7725\n",
      "F1-score (macro)  : 0.7677\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:08<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.7667\n",
      "Precision (macro) : 0.7374\n",
      "Recall   (macro)  : 0.7350\n",
      "F1-score (macro)  : 0.7362\n",
      "\n",
      "--- Fold 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:09<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.7933\n",
      "Precision (macro) : 0.7756\n",
      "Recall   (macro)  : 0.7425\n",
      "F1-score (macro)  : 0.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:09<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST RESULTS =====\n",
      "Accuracy          : 0.7933\n",
      "Precision (macro) : 0.7756\n",
      "Recall   (macro)  : 0.7425\n",
      "F1-score (macro)  : 0.7540\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:53:12.670541Z",
     "start_time": "2025-05-18T15:53:12.101907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Supponiamo che questi siano i punteggi F1 del modello originale (da k-fold CV)\n",
    "f1_model_original = np.array([0.7587, 0.7587, 0.7677, 0.7362, 0.7540])  \n",
    "\n",
    "# Supponiamo che questo sia il punteggio F1 del modello con augmentazione (unico valore)\n",
    "f1_model_variant = 0.7412  \n",
    "\n",
    "# Calcola la media e la deviazione standard del primo modello\n",
    "mean_original = np.mean(f1_model_original)\n",
    "std_original = np.std(f1_model_original, ddof=1)\n",
    "\n",
    "# Test t per un campione (compara il valore del modello variante con la media del primo modello)\n",
    "statistic, p_value = ttest_1samp(f1_model_original, f1_model_variant)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(f\"T-statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretare i risultati\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Il modello variante ha una performance significativamente migliore del modello originale.\")\n",
    "else:\n",
    "    print(\"Non ci sono prove sufficienti per affermare che il modello variante si comporti meglio del modello originale.\")\n"
   ],
   "id": "f91a8405d34828c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 2.6597543348225687\n",
      "P-value: 0.05640480416136819\n",
      "Non ci sono prove sufficienti per affermare che il modello variante si comporti meglio del modello originale.\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
