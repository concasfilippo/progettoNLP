{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Script per eseguire la grid search sul modello clip vit large patch ",
   "id": "11cdc3cfc407ed6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:10:20.011652Z",
     "start_time": "2025-05-18T00:08:23.031080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre computa gli embedding di clip una volta sola e poi addestra il classificatore\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# CONFIG\n",
    "TRAIN_DATA_DIR = \"../pre_processing/dataset/train/\"\n",
    "TRAIN_ANNOTATIONS_PATH = '../pre_processing/dataset/train.json'\n",
    "VAL_DATA_DIR = \"../pre_processing/dataset/val/\"\n",
    "VAL_ANNOTATIONS_PATH = '../pre_processing/dataset/val.json'\n",
    "TEST_DATA_DIR = \"../pre_processing/dataset/test/\"\n",
    "TEST_ANNOTATIONS_PATH = '../pre_processing/dataset/test.json'\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NOME_DEL_MODELLO_PRETRAINED = \"openai/clip-vit-large-patch14\"\n",
    "#NOME_DEL_MODELLO_PRETRAINED = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Iperparametri da esplorare\n",
    "dropouts = [0.0] #[0.0, 0.1, 0.2, 0.3]\n",
    "lrs = [0.01] #[0.01, 0.001, 0.0001]\n",
    "weight_decays = [0] #[0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "schedulers = ['none'] #['none', 'StepLR', 'LinearLR', 'ExponentialLR', 'CosineAnnealingLR']\n",
    "unlock_settings = [False] #[False, True]\n",
    "\n",
    "# MOdello CLIP per la feature extraction\n",
    "clip_model = CLIPModel.from_pretrained(NOME_DEL_MODELLO_PRETRAINED).to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(NOME_DEL_MODELLO_PRETRAINED)\n",
    "clip_model.eval()\n",
    "\n",
    "safe_nome_modello_pretrained = NOME_DEL_MODELLO_PRETRAINED.replace(\"/\", \"_\")\n",
    "\n",
    "# Funzione di utility per caricare le annotazioni\n",
    "def load_annotations(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_annotations = load_annotations(TRAIN_ANNOTATIONS_PATH)\n",
    "val_annotations = load_annotations(VAL_ANNOTATIONS_PATH)\n",
    "test_annotations = load_annotations(TEST_ANNOTATIONS_PATH)\n",
    "\n",
    "all_labels = [a[\"label\"] for a in train_annotations + val_annotations + test_annotations]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# FUnzione per trasformare le immagini\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    \n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset per computare gli embeddings\n",
    "class RawMultimodalDataset(Dataset):\n",
    "    def __init__(self, annotations, img_folder, label_encoder):\n",
    "        self.annotations = annotations\n",
    "        self.img_folder = img_folder\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.annotations[idx]\n",
    "        text = item[\"text\"].lower().replace(\"\\n\", \" \").strip()\n",
    "        \n",
    "        # Pulizia del testo\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\\\n\", \" \", text).strip() \n",
    "        text = re.sub(r\"\\n\", \" \", text).strip() \n",
    "        text = re.sub(r\"\\\\\", \" \", text).strip() \n",
    "        text = re.sub(r\"  \", \" \", text).strip()\n",
    "        \n",
    "        image_path = os.path.join(self.img_folder, item[\"label\"], item[\"image\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform(image)\n",
    "        label = self.label_encoder.transform([item[\"label\"]])[0]\n",
    "        return text, image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Precompute Embeddings\n",
    "def compute_embeddings(dataset):\n",
    "    text_embeds, image_embeds, labels = [], [], []\n",
    "    for text, image, label in tqdm(dataset, desc=\"Computing Embeddings\"):\n",
    "        inputs = clip_processor(text=[text], images=image.unsqueeze(0), return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            clip_out = clip_model(**inputs)\n",
    "        features = torch.cat([clip_out.text_embeds, clip_out.image_embeds], dim=1)\n",
    "        text_embeds.append(features.squeeze(0).cpu())\n",
    "        labels.append(label)\n",
    "    return torch.stack(text_embeds), torch.tensor(labels)\n",
    "\n",
    "# Caricamento dataset\n",
    "train_raw = RawMultimodalDataset(train_annotations, TRAIN_DATA_DIR, label_encoder)\n",
    "val_raw = RawMultimodalDataset(val_annotations, VAL_DATA_DIR, label_encoder)\n",
    "test_raw = RawMultimodalDataset(test_annotations, TEST_DATA_DIR, label_encoder)\n",
    "\n",
    "train_features, train_labels = compute_embeddings(train_raw)\n",
    "val_features, val_labels = compute_embeddings(val_raw)\n",
    "test_features, test_labels = compute_embeddings(test_raw)\n",
    "\n",
    "# Dataset wrapper\n",
    "class PrecomputedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = PrecomputedDataset(train_features, train_labels)\n",
    "val_dataset = PrecomputedDataset(val_features, val_labels)\n",
    "test_dataset = PrecomputedDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Modello di classificazione\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# File in cui trasferire i risultati\n",
    "results_path = f\"{safe_nome_modello_pretrained}_best_and_aug.csv\"\n",
    "with open(results_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['dropout', 'lr', 'weight_decay', 'scheduler', 'unlocked',\n",
    "                     'val_accuracy', 'val_precision', 'val_recall', 'val_f1',\n",
    "                     'test_accuracy', 'test_precision', 'test_recall', 'test_f1'])\n",
    "\n",
    "\n",
    "# Training Loop per Grid Search \n",
    "combinations = list(product(dropouts, lrs, weight_decays, schedulers, unlock_settings))\n",
    "\n",
    "for dropout, lr, wd, sched_name, unlock in tqdm(combinations, desc=\"Grid Search\", leave=True):\n",
    "    \n",
    "    model_name = f\"./saves_model2.2/{safe_nome_modello_pretrained}_drop{dropout}_lrs{lr}_weights_{wd}_sched_{sched_name}_unlock{unlock}_augmented\"\n",
    "    \n",
    "    #model_name = f\"./saves_model2.2/openai_clip_vit_large_patch14_drop{dropout}_lrs{lr}_weights_{wd}_sched_{sched_name}_unlock{unlock}\"\n",
    "    safe_model_name = model_name #.replace(\"/\", \"_\")\n",
    "\n",
    "    #classifier = MultimodalClassifier(input_dim=768+768, dropout=dropout).to(DEVICE)\n",
    "    clip_hidden_dim = clip_model.config.projection_dim\n",
    "    classifier = MultimodalClassifier(input_dim=clip_hidden_dim * 2, dropout=dropout).to(DEVICE)\n",
    "\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if sched_name == 'StepLR':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    elif sched_name == 'LinearLR':\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.9, total_iters=EPOCHS)\n",
    "    elif sched_name == 'ExponentialLR':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif sched_name == 'CosineAnnealingLR':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    if unlock:\n",
    "        clip_model = CLIPModel.from_pretrained(NOME_DEL_MODELLO_PRETRAINED).to(DEVICE)\n",
    "        clip_model.train()\n",
    "    \n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False  # Freeza tutto\n",
    "    \n",
    "        for param in clip_model.vision_model.encoder.layers[-3:].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in clip_model.text_model.encoder.layers[-3:].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in clip_model.vision_model.post_layernorm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in clip_model.text_model.final_layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    #for epoch in trange(EPOCHS, desc=f\"{safe_model_name}\", leave=False):\n",
    "    for epoch in range(EPOCHS):\n",
    "        classifier.train()\n",
    "        epoch_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            logits = classifier(features).squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                logits = classifier(features).squeeze()\n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        classifier.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in loader:\n",
    "                features = features.to(DEVICE)\n",
    "                logits = classifier(features).squeeze()\n",
    "                preds = (logits > 0.5).float()\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        return acc, prec, rec, f1\n",
    "\n",
    "    val_metrics = evaluate(val_loader)\n",
    "    test_metrics = evaluate(test_loader)\n",
    "\n",
    "    with open(results_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([dropout, lr, wd, sched_name, unlock, *val_metrics, *test_metrics])\n",
    "\n",
    "    # Salva loss plot\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss Curve - {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{safe_model_name}_loss_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Salva model\n",
    "    #torch.save(classifier.state_dict(), f\"{safe_model_name}.pt\")\n",
    "    #del classifier\n",
    "    #torch.cuda.empty_cache()\n"
   ],
   "id": "9338c303cf741c7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Computing Embeddings:   0%|          | 0/1200 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Computing Embeddings: 100%|██████████| 1200/1200 [01:11<00:00, 16.73it/s]\n",
      "Computing Embeddings: 100%|██████████| 150/150 [00:11<00:00, 13.44it/s]\n",
      "Computing Embeddings: 100%|██████████| 300/300 [00:23<00:00, 12.88it/s]\n",
      "Grid Search: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
